{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\bryan\\.cache\\kagglehub\\datasets\\antonygarciag\\fall-audio-detection-dataset\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"antonygarciag/fall-audio-detection-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Labeled dataset saved at: C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset\\labeled_dataset.csv\n",
      "               Filename  Label\n",
      "0  01-020-02-073-01.wav      1\n",
      "1  01-022-07-014-01.wav      1\n",
      "2  01-025-00-304-02.wav      0\n",
      "3  01-028-01-028-01.wav      1\n",
      "4  01-029-00-330-02.wav      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset\n",
    "DATASET_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset\"\n",
    "\n",
    "# List all audio files\n",
    "files = [f for f in os.listdir(DATASET_PATH) if f.endswith(\".wav\")]\n",
    "\n",
    "# Extract labels from filenames\n",
    "data = []\n",
    "for file in files:\n",
    "    parts = file.split(\"-\")  # Split by '-'\n",
    "    \n",
    "    if len(parts) >= 5:\n",
    "        label = int(parts[-1].split(\".\")[0])  # Extract FF (last number before .wav)\n",
    "        label = 1 if label == 1 else 0  # Convert to binary (1 = Fall, 0 = Non-Fall)\n",
    "        data.append([file, label])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Filename\", \"Label\"])\n",
    "\n",
    "# Save labeled dataset\n",
    "csv_path = os.path.join(DATASET_PATH, \"labeled_dataset.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"[INFO] Labeled dataset saved at: {csv_path}\")\n",
    "print(df.head())  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled dataset\n",
    "df_labels = pd.read_csv(os.path.join(DATASET_PATH, \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset/labeled_dataset.csv\"))\n",
    "\n",
    "# Create file-label mapping\n",
    "file_to_label = dict(zip(df_labels[\"Filename\"], df_labels[\"Label\"]))\n",
    "\n",
    "# Load dataset with proper labels\n",
    "train_files = []\n",
    "train_labels = []\n",
    "\n",
    "for file in os.listdir(DATASET_PATH):\n",
    "    if file.endswith(\".wav\") and file in file_to_label:\n",
    "        train_files.append(os.path.join(DATASET_PATH, file))\n",
    "        train_labels.append(file_to_label[file])  # Use actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Training samples: 760, Validation samples: 190\n",
      "[INFO] Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n",
      "Epoch 1/20 [Validation]: 100%|██████████| 6/6 [00:03<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Train Loss: 1.0377 | Val Loss: 0.4386 | Val Acc: 0.7842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.15it/s]\n",
      "Epoch 2/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Train Loss: 0.3920 | Val Loss: 0.2893 | Val Acc: 0.8579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.39it/s]\n",
      "Epoch 3/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Train Loss: 0.2764 | Val Loss: 0.2302 | Val Acc: 0.9053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.24it/s]\n",
      "Epoch 4/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Train Loss: 0.2255 | Val Loss: 0.1838 | Val Acc: 0.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.92it/s]\n",
      "Epoch 5/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Train Loss: 0.1996 | Val Loss: 0.1628 | Val Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.07it/s]\n",
      "Epoch 6/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Train Loss: 0.1872 | Val Loss: 0.1475 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.08it/s]\n",
      "Epoch 7/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Train Loss: 0.1721 | Val Loss: 0.1369 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.02it/s]\n",
      "Epoch 8/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Train Loss: 0.1705 | Val Loss: 0.1583 | Val Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.21it/s]\n",
      "Epoch 9/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] - Train Loss: 0.1623 | Val Loss: 0.1271 | Val Acc: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.99it/s]\n",
      "Epoch 10/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] - Train Loss: 0.1571 | Val Loss: 0.1279 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.87it/s]\n",
      "Epoch 11/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] - Train Loss: 0.1537 | Val Loss: 0.1232 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Training]: 100%|██████████| 24/24 [00:03<00:00,  6.05it/s]\n",
      "Epoch 12/20 [Validation]: 100%|██████████| 6/6 [00:00<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] - Train Loss: 0.1522 | Val Loss: 0.1190 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.90it/s]\n",
      "Epoch 13/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] - Train Loss: 0.1502 | Val Loss: 0.1443 | Val Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.78it/s]\n",
      "Epoch 14/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] - Train Loss: 0.1606 | Val Loss: 0.1223 | Val Acc: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.94it/s]\n",
      "Epoch 15/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] - Train Loss: 0.1706 | Val Loss: 0.1405 | Val Acc: 0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.72it/s]\n",
      "Epoch 16/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] - Train Loss: 0.1497 | Val Loss: 0.1583 | Val Acc: 0.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.68it/s]\n",
      "Epoch 17/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] - Train Loss: 0.1459 | Val Loss: 0.1146 | Val Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.44it/s]\n",
      "Epoch 18/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] - Train Loss: 0.1432 | Val Loss: 0.1138 | Val Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.69it/s]\n",
      "Epoch 19/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] - Train Loss: 0.1464 | Val Loss: 0.1122 | Val Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Training]: 100%|██████████| 24/24 [00:04<00:00,  5.70it/s]\n",
      "Epoch 20/20 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] - Train Loss: 0.1420 | Val Loss: 0.1178 | Val Acc: 0.9579\n",
      "[INFO] New model saved at C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall_detection_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Set device (Use GPU if available) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# --- Paths ---\n",
    "DATASET_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset\"\n",
    "LABELS_FILE = os.path.join(DATASET_PATH, \"labeled_dataset.csv\")\n",
    "\n",
    "# --- Load labeled dataset ---\n",
    "try:\n",
    "    df_labels = pd.read_csv(LABELS_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Labels file not found: {LABELS_FILE}\")\n",
    "    exit()\n",
    "\n",
    "file_to_label = dict(zip(df_labels[\"Filename\"], df_labels[\"Label\"]))\n",
    "\n",
    "# --- Feature Extraction: MFCC Only ---\n",
    "def extract_features(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "        # Extract MFCC (Only 20 features)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)  # Take the mean across time axis\n",
    "\n",
    "        return mfcc_mean  # Only MFCC features (20 values)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Failed to extract features from {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Custom Dataset Class ---\n",
    "class FallDataset(Dataset):\n",
    "    def __init__(self, dataset_path, file_to_label):\n",
    "        self.file_paths = list(file_to_label.keys())\n",
    "        self.labels = list(file_to_label.values())\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.dataset_path, self.file_paths[idx])\n",
    "        features = extract_features(file_path)\n",
    "\n",
    "        # Handle missing files\n",
    "        if features is None:\n",
    "            return self.__getitem__((idx + 1) % len(self.file_paths))  # Skip to next sample\n",
    "\n",
    "        label = float(self.labels[idx])  # Ensure labels are float32 for BCELoss\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = FallDataset(DATASET_PATH, file_to_label)\n",
    "\n",
    "# --- Split dataset into training (80%) and validation (20%) ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"[INFO] Training samples: {train_size}, Validation samples: {val_size}\")\n",
    "\n",
    "# --- Define Fall Detection Model (Uses Only MFCC Features) ---\n",
    "class FallDetectionModel(nn.Module):\n",
    "    def __init__(self, input_size=20):  # Only 20 MFCC features\n",
    "        super(FallDetectionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# --- Initialize Model ---\n",
    "fall_model = FallDetectionModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(fall_model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Train Model ---\n",
    "EPOCHS = 20\n",
    "print(\"[INFO] Training started...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fall_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\"):\n",
    "        features, labels = features.to(device), labels.to(device).unsqueeze(1)  # Adjust for BCELoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fall_model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    fall_model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\"):\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = fall_model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \"\n",
    "          f\"Train Loss: {total_train_loss / len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {total_val_loss / len(val_loader):.4f} | \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# --- Save the Trained Model ---\n",
    "MODEL_SAVE_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall_detection_model.pth\"\n",
    "torch.save(fall_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"[INFO] New model saved at {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\AppData\\Local\\Temp\\ipykernel_81792\\288127247.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fall_model.load_state_dict(torch.load(FALL_DETECTION_MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Capturing Audio...\n",
      "[INFO] Recording 7 seconds of audio...\n",
      "[INFO] Recording complete.\n",
      "[DEBUG] Fall Detection Model Output: 0.7413294911384583\n",
      "[INFO] Performing speech recognition...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Emergency detected, alerting caregivers!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Small delay before next capture\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#Capturing recorded audio to detect for fall/non-fall works but the real-time record is a bit inconsistent\u001b[39;00m\n\u001b[0;32m    190\u001b[0m test_audio(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset/01-022-07-014-01.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Constants ---\n",
    "FORMAT = pyaudio.paInt16  # 16-bit audio\n",
    "CHANNELS = 1  # Mono\n",
    "RATE = 16000  # Sampling rate\n",
    "CHUNK = 1024  # Audio chunk size\n",
    "MODEL_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/vosk-model-small-en-us-0.15\"  # Vosk model for speech recognition\n",
    "FALL_DETECTION_MODEL_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall_detection_model.pth\"  # Pretrained fall detection model\n",
    "RECORD_SECONDS = 6  # Duration for each audio capture\n",
    "FALL_THRESHOLD = 0.9  # Increased threshold to reduce false positives\n",
    "\n",
    "# --- Load PyTorch Model for Fall Detection ---\n",
    "class FallDetectionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FallDetectionModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(20, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Load the trained fall detection model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fall_model = FallDetectionModel().to(device)\n",
    "fall_model.load_state_dict(torch.load(FALL_DETECTION_MODEL_PATH, map_location=device))\n",
    "fall_model.eval()\n",
    "\n",
    "# --- Function to Record Audio ---\n",
    "def record_audio(output_filename=\"recorded_audio.wav\"):\n",
    "    print(f\"[INFO] Recording {RECORD_SECONDS} seconds of audio...\")\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        frames.append(stream.read(CHUNK))\n",
    "\n",
    "    print(\"[INFO] Recording complete.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save to file\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def extract_mfcc(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        \n",
    "        # Extract MFCC (Only 20 features)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)  # Take the mean across time axis\n",
    "\n",
    "        \n",
    "        return mfcc_mean  # Only MFCC features (20 values)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract features from {audio_file}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def test_audio(audio_file):\n",
    "    print(f\"[INFO] Testing audio: {audio_file}\")\n",
    "\n",
    "    features = extract_mfcc(audio_file)\n",
    "\n",
    "    if features is None:\n",
    "        print(\"[ERROR] Could not extract features from the audio file.\")\n",
    "        return\n",
    "\n",
    "    tensor_input = torch.tensor(features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(tensor_input).item()\n",
    "\n",
    "    print(f\"[DEBUG] Fall Detection Model Output: {prediction}\")\n",
    "\n",
    "    if prediction > 0.8:\n",
    "        print(\"[ALERT] Fall Detected!\")\n",
    "    else:\n",
    "        print(\"[INFO] No Fall Detected.\")\n",
    "\n",
    "# --- Function to Detect Fall ---\n",
    "def detect_fall(audio_file):\n",
    "    mfcc_features = extract_mfcc(audio_file)\n",
    "    mfcc_tensor = torch.tensor(mfcc_features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(mfcc_tensor).item()\n",
    "\n",
    "    print(f\"[DEBUG] Fall Detection Model Output: {prediction}\")\n",
    "\n",
    "    if prediction > FALL_THRESHOLD:  # Increased threshold\n",
    "        print(\"[ALERT] Fall Detected!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"\n",
    "# --- Function to Perform Speech Recognition ---\n",
    "def recognize_speech(audio_file):\n",
    "    print(\"[INFO] Performing speech recognition...\")\n",
    "    \n",
    "    model = Model(MODEL_PATH)\n",
    "    rec = KaldiRecognizer(model, RATE)\n",
    "\n",
    "    with wave.open(audio_file, \"rb\") as wf:\n",
    "        while True:\n",
    "            data = wf.readframes(CHUNK)\n",
    "            if not data:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                text = result.get(\"text\", \"\").strip()\n",
    "                if text:\n",
    "                    print(f\"[INFO] Recognized Text: {text}\")\n",
    "\n",
    "                    # Check for distress call\n",
    "                    if any(word in text.lower() for word in [\"help\", \"fall\", \"ouch\", \"emergency\"]):\n",
    "                        print(\"[ALERT] Patient asking for help detected!\")\n",
    "                        return text\n",
    "    return None\n",
    "\"\"\"\n",
    "# --- Function to Capture Live Audio ---\n",
    "def capture_live_audio():\n",
    "    print(\"[INFO] Capturing live audio...\")\n",
    "    duration = 5  # Capture for 5 seconds\n",
    "    audio_data = sd.rec(int(duration * RATE), samplerate=RATE, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    return audio_data\n",
    "\n",
    "# --- Function to Process Live Audio for Fall Detection ---\n",
    "def analyze_live_audio():\n",
    "    audio_data = capture_live_audio()\n",
    "    audio_data = np.squeeze(audio_data)  # Remove unnecessary dimensions\n",
    "    mfcc_features = librosa.feature.mfcc(y=audio_data.astype(float), sr=RATE, n_mfcc=20)\n",
    "    mfcc_features = np.mean(mfcc_features, axis=1)\n",
    "\n",
    "    mfcc_tensor = torch.tensor(mfcc_features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(mfcc_tensor).item()\n",
    "\n",
    "    if prediction > 0.8:\n",
    "        print(\"[ALERT] Fall Detected in Live Audio!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# --- Main Loop ---\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"\\n[INFO] Capturing Audio...\")\n",
    "        recorded_file = record_audio()  # Capture audio\n",
    "\n",
    "        # Fall Detection\n",
    "        fall_detected = detect_fall(recorded_file)\n",
    "        if fall_detected:\n",
    "            print(\"[INFO] Sending fall alert to Raspberry Pi...\")\n",
    "            break  # Stop loop on fall detection\n",
    "\n",
    "        # Speech Recognition\n",
    "        distress_text = recognize_speech(recorded_file)\n",
    "        if distress_text:\n",
    "            print(\"[INFO] Emergency detected, alerting caregivers!\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Small delay before next capture\n",
    "        \n",
    "        #Capturing recorded audio to detect for fall/non-fall works but the real-time record is a bit inconsistent\n",
    "        test_audio(\"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset/01-022-07-014-01.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
