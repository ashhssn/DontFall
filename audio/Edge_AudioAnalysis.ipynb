{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\bryan\\.cache\\kagglehub\\datasets\\antonygarciag\\fall-audio-detection-dataset\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"antonygarciag/fall-audio-detection-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Labeled dataset saved at: C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset\\labeled_dataset.csv\n",
      "               Filename  Label\n",
      "0  01-020-02-073-01.wav      1\n",
      "1  01-022-07-014-01.wav      1\n",
      "2  01-025-00-304-02.wav      0\n",
      "3  01-028-01-028-01.wav      1\n",
      "4  01-029-00-330-02.wav      0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset\n",
    "DATASET_PATH = \"C:/Users/bryan/OneDrive/Desktop/Project/DontFall/audio/fall-audio-detection-dataset\"\n",
    "\n",
    "# List all audio files\n",
    "files = [f for f in os.listdir(DATASET_PATH) if f.endswith(\".wav\")]\n",
    "\n",
    "# Extract labels from filenames\n",
    "data = []\n",
    "for file in files:\n",
    "    parts = file.split(\"-\")  # Split by '-'\n",
    "    \n",
    "    if len(parts) >= 5:\n",
    "        label = int(parts[-1].split(\".\")[0])  # Extract FF (last number before .wav)\n",
    "        label = 1 if label == 1 else 0  # Convert to binary (1 = Fall, 0 = Non-Fall)\n",
    "        data.append([file, label])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Filename\", \"Label\"])\n",
    "\n",
    "# Save labeled dataset\n",
    "csv_path = os.path.join(DATASET_PATH, \"labeled_dataset.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"[INFO] Labeled dataset saved at: {csv_path}\")\n",
    "print(df.head())  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled dataset\n",
    "df_labels = pd.read_csv(os.path.join(DATASET_PATH, \"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/labeled_dataset.csv\"))\n",
    "\n",
    "# Create file-label mapping\n",
    "file_to_label = dict(zip(df_labels[\"Filename\"], df_labels[\"Label\"]))\n",
    "\n",
    "# Load dataset with proper labels\n",
    "train_files = []\n",
    "train_labels = []\n",
    "\n",
    "for file in os.listdir(DATASET_PATH):\n",
    "    if file.endswith(\".wav\") and file in file_to_label:\n",
    "        train_files.append(os.path.join(DATASET_PATH, file))\n",
    "        train_labels.append(file_to_label[file])  # Use actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Training samples: 760, Validation samples: 190\n",
      "[INFO] Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Set device (Use GPU if available) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# --- Paths ---\n",
    "DATASET_PATH = \"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/fall-audio-detection-dataset\"\n",
    "LABELS_FILE = os.path.join(DATASET_PATH, \"labeled_dataset.csv\")\n",
    "\n",
    "# --- Load labeled dataset ---\n",
    "try:\n",
    "    df_labels = pd.read_csv(LABELS_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Labels file not found: {LABELS_FILE}\")\n",
    "    exit()\n",
    "\n",
    "file_to_label = dict(zip(df_labels[\"Filename\"], df_labels[\"Label\"]))\n",
    "\n",
    "# --- Feature Extraction: MFCC Only ---\n",
    "def extract_features(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "        # Extract MFCC (Only 20 features)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)  # Take the mean across time axis\n",
    "\n",
    "        return mfcc_mean  # Only MFCC features (20 values)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Failed to extract features from {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Custom Dataset Class ---\n",
    "class FallDataset(Dataset):\n",
    "    def __init__(self, dataset_path, file_to_label):\n",
    "        self.file_paths = list(file_to_label.keys())\n",
    "        self.labels = list(file_to_label.values())\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.dataset_path, self.file_paths[idx])\n",
    "        features = extract_features(file_path)\n",
    "\n",
    "        # Handle missing files\n",
    "        if features is None:\n",
    "            return self.__getitem__((idx + 1) % len(self.file_paths))  # Skip to next sample\n",
    "\n",
    "        label = float(self.labels[idx])  # Ensure labels are float32 for BCELoss\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = FallDataset(DATASET_PATH, file_to_label)\n",
    "\n",
    "# --- Split dataset into training (80%) and validation (20%) ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"[INFO] Training samples: {train_size}, Validation samples: {val_size}\")\n",
    "\n",
    "# --- Define Fall Detection Model (Uses Only MFCC Features) ---\n",
    "class FallDetectionModel(nn.Module):\n",
    "    def __init__(self, input_size=20):  # Only 20 MFCC features\n",
    "        super(FallDetectionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# --- Initialize Model ---\n",
    "fall_model = FallDetectionModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(fall_model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Train Model ---\n",
    "EPOCHS = 20\n",
    "print(\"[INFO] Training started...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fall_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\"):\n",
    "        features, labels = features.to(device), labels.to(device).unsqueeze(1)  # Adjust for BCELoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fall_model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # --- Validation Loop ---\n",
    "    fall_model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\"):\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = fall_model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \"\n",
    "          f\"Train Loss: {total_train_loss / len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {total_val_loss / len(val_loader):.4f} | \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# --- Save the Trained Model ---\n",
    "MODEL_SAVE_PATH = \"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/fall_detection_model.pth\"\n",
    "torch.save(fall_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"[INFO] New model saved at {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vosk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvosk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model, KaldiRecognizer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vosk'"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Constants ---\n",
    "FORMAT = pyaudio.paInt16  # 16-bit audio\n",
    "CHANNELS = 1  # Mono\n",
    "RATE = 16000  # Sampling rate\n",
    "CHUNK = 1024  # Audio chunk size\n",
    "MODEL_PATH = \"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/vosk-model-small-en-us-0.15\"  # Vosk model for speech recognition\n",
    "FALL_DETECTION_MODEL_PATH = \"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/fall_detection_model.pth\"  # Pretrained fall detection model\n",
    "RECORD_SECONDS = 7  # Duration for each audio capture\n",
    "FALL_THRESHOLD = 0.9  # Increased threshold to reduce false positives\n",
    "\n",
    "# --- Load PyTorch Model for Fall Detection ---\n",
    "class FallDetectionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FallDetectionModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(20, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Load the trained fall detection model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fall_model = FallDetectionModel().to(device)\n",
    "fall_model.load_state_dict(torch.load(FALL_DETECTION_MODEL_PATH, map_location=device))\n",
    "fall_model.eval()\n",
    "\n",
    "# --- Function to Record Audio ---\n",
    "def record_audio(output_filename=\"recorded_audio.wav\"):\n",
    "    print(f\"[INFO] Recording {RECORD_SECONDS} seconds of audio...\")\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        frames.append(stream.read(CHUNK))\n",
    "\n",
    "    print(\"[INFO] Recording complete.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save to file\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def extract_mfcc(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "        # Extract MFCC (Only 20 features)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)  # Take the mean across time axis\n",
    "\n",
    "        return mfcc_mean  # Only MFCC features (20 values)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract features from {audio_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_audio(audio_file):\n",
    "    print(f\"[INFO] Testing audio: {audio_file}\")\n",
    "\n",
    "    features = extract_mfcc(audio_file)\n",
    "\n",
    "    if features is None:\n",
    "        print(\"[ERROR] Could not extract features from the audio file.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[DEBUG] Extracted MFCC Features (First 10 Values): {features[:10]}\")\n",
    "\n",
    "    tensor_input = torch.tensor(features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(tensor_input).item()\n",
    "\n",
    "    print(f\"[DEBUG] Fall Detection Model Output: {prediction}\")\n",
    "\n",
    "    if prediction > 0.8:\n",
    "        print(\"[ALERT] Fall Detected!\")\n",
    "    else:\n",
    "        print(\"[INFO] No Fall Detected.\")\n",
    "\n",
    "# --- Function to Detect Fall ---\n",
    "def detect_fall(audio_file):\n",
    "    mfcc_features = extract_mfcc(audio_file)\n",
    "    mfcc_tensor = torch.tensor(mfcc_features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(mfcc_tensor).item()\n",
    "\n",
    "    print(f\"[DEBUG] Fall Detection Model Output: {prediction}\")\n",
    "\n",
    "    if prediction > FALL_THRESHOLD:  # Increased threshold\n",
    "        print(\"[ALERT] Fall Detected!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# --- Function to Perform Speech Recognition ---\n",
    "def recognize_speech(audio_file):\n",
    "    print(\"[INFO] Performing speech recognition...\")\n",
    "    \n",
    "    model = Model(MODEL_PATH)\n",
    "    rec = KaldiRecognizer(model, RATE)\n",
    "\n",
    "    with wave.open(audio_file, \"rb\") as wf:\n",
    "        while True:\n",
    "            data = wf.readframes(CHUNK)\n",
    "            if not data:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                text = result.get(\"text\", \"\").strip()\n",
    "                if text:\n",
    "                    print(f\"[INFO] Recognized Text: {text}\")\n",
    "\n",
    "                    # Check for distress call\n",
    "                    if any(word in text.lower() for word in [\"help\", \"fall\", \"ouch\", \"emergency\"]):\n",
    "                        print(\"[ALERT] Patient asking for help detected!\")\n",
    "                        return text\n",
    "    return None\n",
    "\n",
    "# --- Function to Capture Live Audio ---\n",
    "def capture_live_audio():\n",
    "    print(\"[INFO] Capturing live audio...\")\n",
    "    duration = 5  # Capture for 5 seconds\n",
    "    audio_data = sd.rec(int(duration * RATE), samplerate=RATE, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    return audio_data\n",
    "\n",
    "# --- Function to Process Live Audio for Fall Detection ---\n",
    "def analyze_live_audio():\n",
    "    audio_data = capture_live_audio()\n",
    "    audio_data = np.squeeze(audio_data)  # Remove unnecessary dimensions\n",
    "    mfcc_features = librosa.feature.mfcc(y=audio_data.astype(float), sr=RATE, n_mfcc=20)\n",
    "    mfcc_features = np.mean(mfcc_features, axis=1)\n",
    "\n",
    "    mfcc_tensor = torch.tensor(mfcc_features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = fall_model(mfcc_tensor).item()\n",
    "\n",
    "    if prediction > 0.8:\n",
    "        print(\"[ALERT] Fall Detected in Live Audio!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# --- Main Loop ---\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"\\n[INFO] Capturing Audio...\")\n",
    "        recorded_file = record_audio()  # Capture audio\n",
    "\n",
    "        # Fall Detection\n",
    "        fall_detected = detect_fall(recorded_file)\n",
    "        if fall_detected:\n",
    "            print(\"[INFO] Sending fall alert to Raspberry Pi...\")\n",
    "            break  # Stop loop on fall detection\n",
    "\n",
    "        # Speech Recognition\n",
    "        distress_text = recognize_speech(recorded_file)\n",
    "        if distress_text:\n",
    "            print(\"[INFO] Emergency detected, alerting caregivers!\")\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Small delay before next capture\n",
    "        \n",
    "        test_audio(\"C:/Users/bryan/OneDrive/Desktop/Uni/Edge Computing & Analytics/Project/fall-audio-detection-dataset/01-025-00-304-02.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
